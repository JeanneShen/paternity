{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from re import match\n",
    "import pysam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from convert import convert_hap_samples_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(data_path):\n",
    "    '''从文件夹中，得到各文件的路径'''\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            hap_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.sample'):\n",
    "            samples_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.vcf'):\n",
    "            vcf_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.ped'):\n",
    "            ped_file = os.path.join(data_path,filename)\n",
    "    assert (hap_file and samples_file and vcf_file and ped_file),'missing data file'\n",
    "    return  hap_file,samples_file,vcf_file,ped_file\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simulate generations of families and populations\")\n",
    "    parser.add_argument(\"dataset\", help=\"Input data file with initial individuals\")\n",
    "    parser.add_argument(\"output_file_path\", help=\"path for saving simulation files \")\n",
    "    parser.add_argument(\"-g\", \"--num_generations\", type=int, default=3, help=\"Number of generations to simulate (default: 3)\")\n",
    "    parser.add_argument(\"-c\", \"--num_couples\", type=int, default=10, help=\"Number of initial couples (default: 10)\")\n",
    "    parser.add_argument(\"-l\", \"--num_linkages\", type=int, default=1000, help=\"Number of consecutive non-recombining positions (default: 1000)\")\n",
    "    parser.add_argument(\"-p\", \"--num_keys\", type=int, default=0, help=\"key points for linkage interval (default: 0)\")\n",
    "    parser.add_argument(\"-k\", \"--num_kids\", type=int, default=2, help=\"Number of child for every couple (default: 2)\")\n",
    "    parser.add_argument(\"-r\", \"--num_recombination\", type=int, default=5, help=\"Number of intervals for recombination (default: 5)\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def choice_person(df_data,num_couples,start,end):\n",
    "    res0 = pd.DataFrame()\n",
    "    select_people = {}\n",
    "    num_people = df_data.shape[0]//2\n",
    "    person_list = df_data.index.tolist()\n",
    "    df_data.columns = np.arange(df_data.shape[1])\n",
    "    for i in range(2*num_couples):\n",
    "        index = random.randint(0,num_people)\n",
    "        select_people[i] = person_list[2*index]\n",
    "        select_data = deepcopy(df_data.iloc[index*2:index*2+2])\n",
    "        select_data['personID'] = [i,i]\n",
    "        res0 = pd.concat([res0,select_data])\n",
    "    res0.set_index(np.arange(4*num_couples),drop=True,inplace=True)\n",
    "    for i in range(res0.shape[0]):\n",
    "        res0.iloc[i,start:end] = res0.iloc[i,start]\n",
    "    return res0,select_people\n",
    "\n",
    "def generate_linkage_intervals(num_linkage,num_locus,num_keys=0):\n",
    "    linkage_init = random.randint(0,num_locus-num_linkage)\n",
    "    keys = np.random.choice(np.arange(1,num_linkage-1), num_keys,replace=False) + linkage_init\n",
    "    linkage_keys = np.sort(np.append(keys,[linkage_init,linkage_init+num_linkage+1]))\n",
    "    return linkage_keys\n",
    "\n",
    "def generate_recombination_intervals(num_recombinations,num_locus,linkage_keys):\n",
    "    start = linkage_keys[0]\n",
    "    end = linkage_keys[-1]\n",
    "    num_linkage = end-start\n",
    "    interval1 = range(0,start)\n",
    "    interval2 = range(end+1,num_locus+1)\n",
    "    points1 = start*num_recombinations//(num_locus-num_linkage)\n",
    "    points2 = num_recombinations - points1\n",
    "    recombination_points1 = sorted(random.sample(interval1, 2 * points1)) \n",
    "    recombination_points2 = sorted(random.sample(interval2, 2 * points2))\n",
    "    recombination_points = recombination_points1 + recombination_points2\n",
    "    recombination_intervals = [(recombination_points[i], recombination_points[i + 1]) for i in range(0, len(recombination_points), 2)]\n",
    "    return recombination_intervals\n",
    "\n",
    "def is_variant_in_recombination_intervals(variant_pos, recombination_intervals):\n",
    "    '''检查一个变异位点是否在给定的重组区间内'''\n",
    "    if recombination_intervals!=[]:\n",
    "        for start, end in recombination_intervals:\n",
    "            if start <= variant_pos <= end:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def sim_meosis(res,num_recombinations,linkage_keys):\n",
    "    num_locus = res.shape[1]-1\n",
    "    start = linkage_keys[0]\n",
    "    end = linkage_keys[-1]\n",
    "    temp = deepcopy(res[res.index%2==0])\n",
    "    for i in range(temp.shape[0]):\n",
    "        if num_recombinations>0:\n",
    "            recombination_intervals = generate_recombination_intervals(num_recombinations,num_locus,linkage_keys)\n",
    "        else:\n",
    "            recombination_intervals = []\n",
    "        for j in range(num_locus):\n",
    "            if j<start or j>=end:\n",
    "                if is_variant_in_recombination_intervals(j,recombination_intervals):\n",
    "                    temp.iloc[i,j] = res.iloc[i+1,j]\n",
    "            # else:\n",
    "            #     # index = np.searchsorted(linkage_keys,j,side='right') - 1   # type: ignore\n",
    "            #     temp.iloc[i,j] = res.iloc[i,start]\n",
    "\n",
    "    return temp\n",
    "\n",
    "def couple_legal(person1,person2,family=None):\n",
    "    if not family:\n",
    "        return True\n",
    "    else:\n",
    "        if person1 in family.keys() and person2 in family.keys():\n",
    "            if family[person1] == family[person2]:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def select_parent(person_rest,family):\n",
    "    persons = deepcopy(person_rest)\n",
    "    parent1,parent2 = random.sample(persons,2)\n",
    "    while not couple_legal(parent1,parent2,family):\n",
    "        parent1,parent2 = random.sample(persons,2)\n",
    "    persons.remove(parent1)\n",
    "    persons.remove(parent2)\n",
    "    return persons,parent1,parent2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generation = 1\n",
    "num_couple = 5\n",
    "num_child = 2\n",
    "num_linkage = 1000\n",
    "num_recombination = 0\n",
    "num_keys = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74560\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/anran/paternity/family-data/sim_seg1\" \n",
    "hap_file,samples_file,_,_=get_file(data_path)\n",
    "df_data = convert_hap_samples_to_dataframe(hap_file,samples_file)\n",
    "df_data.drop(['ID','REF','ALT'],axis=0,inplace=True)\n",
    "num_locus = df_data.shape[1]\n",
    "print(num_locus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49310, 50311)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkage_keys = generate_linkage_intervals(num_linkage,num_locus,num_keys)\n",
    "start = linkage_keys[0]\n",
    "end = linkage_keys[-1]\n",
    "start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0,family = choice_person(df_data,num_couple,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>49310</th>\n",
       "      <th>49311</th>\n",
       "      <th>49312</th>\n",
       "      <th>49313</th>\n",
       "      <th>49314</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   49310 49311 49312 49313 49314\n",
       "0      0     0     0     0     0\n",
       "1      1     1     1     1     1\n",
       "2      1     1     1     1     1\n",
       "3      0     0     0     0     0\n",
       "4      1     1     1     1     1\n",
       "5      0     0     0     0     0\n",
       "6      0     0     0     0     0\n",
       "7      0     0     0     0     0\n",
       "8      1     1     1     1     1\n",
       "9      0     0     0     0     0\n",
       "10     0     0     0     0     0\n",
       "11     0     0     0     0     0\n",
       "12     0     0     0     0     0\n",
       "13     0     0     0     0     0\n",
       "14     0     0     0     0     0\n",
       "15     0     0     0     0     0\n",
       "16     0     0     0     0     0\n",
       "17     0     0     0     0     0\n",
       "18     1     1     1     1     1\n",
       "19     0     0     0     0     0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res0.iloc[:,start:start+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  0  2\n",
       "1  3  3  5\n",
       "2  6  6  8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    df.iloc[i,0:2] = df.iloc[i,0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "type(list(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法ipad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os import replace\n",
    "\n",
    "def choice_person(df_data,num_couples):\n",
    "    res0 = pd.DataFrame()\n",
    "    select_people = {}\n",
    "    num_people = df_data.shape[0]//2\n",
    "    person_list = df_data.index.tolist()\n",
    "    df_data.columns = np.arange(df_data.shape[1])\n",
    "    for i in range(2*num_couples):\n",
    "        index = random.randint(0,num_people)\n",
    "        select_people[i] = person_list[2*index]\n",
    "        select_data = deepcopy(df_data.iloc[index*2:index*2+2])\n",
    "        select_data['personID'] = [i,i]\n",
    "        res0 = pd.concat([res0,select_data])\n",
    "    res0.set_index(np.arange(4*num_couples),drop=True,inplace=True)\n",
    "    return res0,select_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linkage_intervals(num_linkage,num_locus):\n",
    "    '''生成给定数量的重组间隔，即随机选择一定数量的变异位点作为重组点，形成相邻两个变异位点之间的重组区间。'''\n",
    "    linkage_init = random.randint(0,num_locus-num_linkage)\n",
    "    linkage_interval = np.arange(num_linkage)+linkage_init\n",
    "    return linkage_interval\n",
    "\n",
    "def generate_recombination_intervals(num_recombinations,num_locus,linkage_interval):\n",
    "    num_linkage = len(linkage_interval)\n",
    "    interval1 = range(0,linkage_interval[0])\n",
    "    interval2 = range(linkage_interval[-1]+1,num_locus+1)\n",
    "    points1 = linkage_interval[0]*num_recombinations//(num_locus-num_linkage)\n",
    "    points2 = num_recombinations - points1\n",
    "    recombination_points1 = sorted(random.sample(interval1, 2 * points1)) \n",
    "    recombination_points2 = sorted(random.sample(interval2, 2 * points2))\n",
    "    recombination_points = recombination_points1 + recombination_points2\n",
    "    recombination_intervals = [(recombination_points[i], recombination_points[i + 1]) for i in range(0, len(recombination_points), 2)]\n",
    "    return recombination_intervals\n",
    "\n",
    "def is_variant_in_recombination_intervals(variant_pos, recombination_intervals):\n",
    "    '''检查一个变异位点是否在给定的重组区间内'''\n",
    "    for start, end in recombination_intervals:\n",
    "        if start <= variant_pos <= end:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_meosis(res,num_recombinations,linkage_interval):\n",
    "    num_locus = res.shape[1]-1\n",
    "    temp = deepcopy(res[res.index%2==0])\n",
    "    for i in range(temp.shape[0]):\n",
    "        recombination_intervals = generate_recombination_intervals(num_recombinations,num_locus,linkage_interval)\n",
    "        for j in range(num_locus):\n",
    "            if is_variant_in_recombination_intervals(j,recombination_intervals):\n",
    "                temp.iloc[i,j] = res.iloc[i+1,j]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple_legal(person1,person2,family=None):\n",
    "    if not family:\n",
    "        return True\n",
    "    else:\n",
    "        if person1 in family.keys() and person2 in family.keys():\n",
    "            if family[person1] == family[person2]:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def select_parent(person_rest,family):\n",
    "    persons = deepcopy(person_rest)\n",
    "    parent1,parent2 = random.sample(persons,2)\n",
    "    while not couple_legal(parent1,parent2,family):\n",
    "        parent1,parent2 = random.sample(persons,2)\n",
    "    persons.remove(parent1)\n",
    "    persons.remove(parent2)\n",
    "    return persons,parent1,parent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some():\n",
    "    # args = parse_args()\n",
    "\n",
    "    num_generation = 2\n",
    "    num_couple = 3\n",
    "    num_child = 2\n",
    "    family = {}\n",
    "    result_df = pd.DataFrame()\n",
    "    num_linkage = 100\n",
    "    num_recombination = 5\n",
    "\n",
    "    data_path = \"/home/anran/paternity/family-data/sim_seg1\" \n",
    "    hap_file,samples_file,_,_=get_file(data_path)\n",
    "    df_data = convert_hap_samples_to_dataframe(hap_file,samples_file)\n",
    "    df_data.drop(['ID','REF','ALT'],axis=0,inplace=True)\n",
    "    num_locus = df_data.shape[1]\n",
    "    \n",
    "    linkage_interval = generate_linkage_intervals(num_linkage,num_locus)\n",
    "    # 初代的人数据\n",
    "    res0,select_people = choice_person(df_data,num_couples)\n",
    "    print(select_people)\n",
    "    result_df = pd.concat([result_df,res0])\n",
    "    personID = 2*num_couples\n",
    "    # print(res0.shape,result_df.shape,personID)\n",
    "\n",
    "    for g in range(num_generation):\n",
    "        print('#####################',g)\n",
    "        for k in range(num_child):\n",
    "            print('#########',k)\n",
    "            temp = sim_meosis(res0,num_recombination,linkage_interval)\n",
    "            persons_rest = list(temp['personID'].values)\n",
    "            # print(temp.shape,persons_rest)\n",
    "            while persons_rest!=[]:\n",
    "                persons_rest,parent1,parent2 = select_parent(persons_rest,family)\n",
    "                # print('-----------------',parent1,parent2,persons_rest)\n",
    "                family[personID] = [parent1,parent2]\n",
    "                res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "                res_temp['personID'] = [personID,personID]\n",
    "                result_df = pd.concat([result_df,res_temp])\n",
    "                print(result_df.shape)\n",
    "                personID += 1\n",
    "        # display(result_df)\n",
    "        res0 = result_df.iloc[2*personID-4*num_couple:2*personID-2]\n",
    "        # display(res0)\n",
    "    return result_df,res0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generation = 5\n",
    "num_couple = 10\n",
    "num_child = 2\n",
    "# family = {}\n",
    "result_df = pd.DataFrame()\n",
    "num_linkage = 1000\n",
    "num_recombination = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/anran/paternity/family-data/sim_seg1\" \n",
    "hap_file,samples_file,_,_=get_file(data_path)\n",
    "df_data = convert_hap_samples_to_dataframe(hap_file,samples_file)\n",
    "df_data.drop(['ID','REF','ALT'],axis=0,inplace=True)\n",
    "num_locus = df_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'HG03120_0', 1: 'NA21118_0', 2: 'HG01538_0', 3: 'HG02104_0', 4: 'HG01255_0', 5: 'HG03577_0', 6: 'HG03196_0', 7: 'HG00446_0', 8: 'HG02604_0', 9: 'HG01098_0', 10: 'HG00188_0', 11: 'HG02838_0', 12: 'HG01488_0', 13: 'HG03499_0', 14: 'HG00582_0', 15: 'HG04160_0', 16: 'HG00383_0', 17: 'HG02972_0', 18: 'HG02892_0', 19: 'HG03899_0'}\n"
     ]
    }
   ],
   "source": [
    "linkage_interval = generate_linkage_intervals(num_linkage,num_locus)\n",
    "# 初代的人数据\n",
    "res0,family = choice_person(df_data,num_couple)\n",
    "print(family)\n",
    "result_df = pd.concat([result_df,res0])\n",
    "personID = 2*num_couple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### 0\n",
      "######### 0\n",
      "----------------- 4 1 [0, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "----------------- 0 17 [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19]\n",
      "----------------- 14 10 [2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19]\n",
      "----------------- 9 19 [2, 3, 5, 6, 7, 8, 11, 12, 13, 15, 16, 18]\n",
      "----------------- 15 6 [2, 3, 5, 7, 8, 11, 12, 13, 16, 18]\n",
      "----------------- 11 8 [2, 3, 5, 7, 12, 13, 16, 18]\n",
      "----------------- 5 16 [2, 3, 7, 12, 13, 18]\n",
      "----------------- 13 3 [2, 7, 12, 18]\n",
      "----------------- 18 7 [2, 12]\n",
      "----------------- 12 2 []\n",
      "######### 1\n",
      "##################### 1\n",
      "######### 0\n",
      "----------------- 33 29 [20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39]\n",
      "----------------- 34 21 [20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 35, 36, 37, 38, 39]\n",
      "----------------- 22 25 [20, 23, 24, 26, 27, 28, 30, 31, 32, 35, 36, 37, 38, 39]\n",
      "----------------- 36 20 [23, 24, 26, 27, 28, 30, 31, 32, 35, 37, 38, 39]\n",
      "----------------- 35 24 [23, 26, 27, 28, 30, 31, 32, 37, 38, 39]\n",
      "----------------- 32 28 [23, 26, 27, 30, 31, 37, 38, 39]\n",
      "----------------- 27 38 [23, 26, 30, 31, 37, 39]\n",
      "----------------- 39 30 [23, 26, 31, 37]\n",
      "----------------- 26 23 [31, 37]\n",
      "----------------- 31 37 []\n",
      "######### 1\n",
      "##################### 2\n",
      "######### 0\n",
      "----------------- 53 40 [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59]\n",
      "----------------- 58 43 [41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59]\n",
      "----------------- 46 45 [41, 42, 44, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59]\n",
      "----------------- 49 55 [41, 42, 44, 47, 48, 50, 51, 52, 54, 56, 57, 59]\n",
      "----------------- 42 56 [41, 44, 47, 48, 50, 51, 52, 54, 57, 59]\n",
      "----------------- 50 48 [41, 44, 47, 51, 52, 54, 57, 59]\n",
      "----------------- 54 59 [41, 44, 47, 51, 52, 57]\n",
      "----------------- 41 44 [47, 51, 52, 57]\n",
      "----------------- 52 57 [47, 51]\n",
      "----------------- 51 47 []\n",
      "######### 1\n",
      "##################### 3\n",
      "######### 0\n",
      "----------------- 70 71 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "----------------- 62 66 [60, 61, 63, 64, 65, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "----------------- 76 74 [60, 61, 63, 64, 65, 67, 68, 69, 72, 73, 75, 77, 78, 79]\n",
      "----------------- 69 65 [60, 61, 63, 64, 67, 68, 72, 73, 75, 77, 78, 79]\n",
      "----------------- 73 75 [60, 61, 63, 64, 67, 68, 72, 77, 78, 79]\n",
      "----------------- 79 63 [60, 61, 64, 67, 68, 72, 77, 78]\n",
      "----------------- 68 67 [60, 61, 64, 72, 77, 78]\n",
      "----------------- 60 61 [64, 72, 77, 78]\n",
      "----------------- 72 64 [77, 78]\n",
      "----------------- 78 77 []\n",
      "######### 1\n",
      "##################### 4\n",
      "######### 0\n",
      "----------------- 85 97 [80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99]\n",
      "----------------- 83 89 [80, 81, 82, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99]\n",
      "----------------- 84 86 [80, 81, 82, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99]\n",
      "----------------- 81 88 [80, 82, 87, 90, 91, 92, 93, 94, 95, 96, 98, 99]\n",
      "----------------- 90 95 [80, 82, 87, 91, 92, 93, 94, 96, 98, 99]\n",
      "----------------- 98 91 [80, 82, 87, 92, 93, 94, 96, 99]\n",
      "----------------- 94 96 [80, 82, 87, 92, 93, 99]\n",
      "----------------- 80 93 [82, 87, 92, 99]\n",
      "----------------- 87 92 [82, 99]\n",
      "----------------- 99 82 []\n",
      "######### 1\n"
     ]
    }
   ],
   "source": [
    "for g in range(num_generation):\n",
    "    print('#####################',g)\n",
    "    couple = []\n",
    "    for k in range(num_child):\n",
    "        print('#########',k)\n",
    "        temp = sim_meosis(res0,num_recombination,linkage_interval)\n",
    "        persons_rest = list(map(int,list(temp['personID'].values)))\n",
    "        # print(temp.shape,persons_rest)\n",
    "        if k==0:\n",
    "            while persons_rest!=[]:\n",
    "                persons_rest,parent1,parent2 = select_parent(persons_rest,family)\n",
    "                print('-----------------',parent1,parent2,persons_rest)\n",
    "                couple.append((parent1,parent2))\n",
    "                family[personID] = [parent1,parent2]\n",
    "                res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "                res_temp['personID'] = [personID,personID]\n",
    "                result_df = pd.concat([result_df,res_temp])\n",
    "                personID += 1\n",
    "        else:\n",
    "            for (parent1,parent2) in couple:\n",
    "                family[personID] = [parent1,parent2]\n",
    "                res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "                res_temp['personID'] = [personID,personID]\n",
    "                result_df = pd.concat([result_df,res_temp])\n",
    "                personID += 1\n",
    "    #     display(result_df)\n",
    "    res0 = result_df.iloc[2*personID-4*num_couple:2*personID]\n",
    "    res0.set_index(np.arange(res0.shape[0]),drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "family['start'] = int(linkage_interval[0])\n",
    "family['end'] = int(linkage_interval[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('/home/anran/paternity/family-data/sim_data/data2.csv')\n",
    "# flist=[family]\n",
    "with open(\"/home/anran/paternity/family-data/sim_data/info2.json\",\"w\",encoding='utf-8') as f: ## 设置'utf-8'编码\n",
    "    # f.write(json.dumps(family, ensure_ascii=False) + '\\n') \n",
    "    json.dump(family,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- py文件验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import random\n",
    "from convert import convert_hap_samples_to_dataframe\n",
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import numba\n",
    "import os\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import numpy2ri\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "K=10\n",
    "\n",
    "\n",
    "def get_file(data_path):\n",
    "    '''从文件夹中，得到各文件的路径'''\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            hap_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.sample'):\n",
    "            samples_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.vcf'):\n",
    "            vcf_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.ped'):\n",
    "            ped_file = os.path.join(data_path,filename)\n",
    "    assert (hap_file and samples_file and vcf_file and ped_file),'missing data file'\n",
    "    return  hap_file,samples_file,vcf_file,ped_file\n",
    "\n",
    "'''\n",
    "# def select_locus(vcf, n, father, mother, child):\n",
    "#     # 从vcf中随机抽出父母和孩子n行的数据\n",
    "#     df = pd.DataFrame(columns=['locus','moGT','faGT', 'chGT','f0'])\n",
    "#     for record in vcf:   # 先将所有数据读进df\n",
    "#         locus = (record.chrom,str(record.pos))\n",
    "#         f0 = 1-record.info['AF'][0]\n",
    "#         if f0<1:  # 除去f0=1的情况，不然会导致之后 arctanh(1)=inf\n",
    "#             mo = record.samples[mother]['GT']\n",
    "#             ch = record.samples[child]['GT']\n",
    "#             if (mo!=(0,1) and mo!=(1,0)) or (ch!=(0,1) and ch!=(1,0)):\n",
    "#                 fa = record.samples[father]['GT']\n",
    "#                 df.loc[len(df.index)] = [locus,mo,fa,ch,f0] # type: ignore\n",
    "#     num_row = df.shape[0]\n",
    "#     assert num_row>=n,'not enough locus to select'\n",
    "#     df_select = df.sample(n,replace=False,axis=0)\n",
    "#     df_select.set_index('locus',drop=True,inplace=True)\n",
    "#     return df_select\n",
    "\n",
    "# def select_locus(vcf, n, father, mother, child):\n",
    "#     # 从vcf中随机抽出<<连续的父母>>和孩子n行的数据\n",
    "#     df = pd.DataFrame(columns=['locus','moGT','faGT', 'chGT','f0'])\n",
    "#     for record in vcf:   # 先将所有数据读进df\n",
    "#         locus = (record.chrom,str(record.pos))\n",
    "#         f0 = 1-record.info['AF'][0]\n",
    "#         if f0<1:  # 除去f0=1的情况，不然会导致之后 arctanh(1)=inf\n",
    "#             mo = record.samples[mother]['GT']\n",
    "#             ch = record.samples[child]['GT']\n",
    "#             if (mo!=(0,1) and mo!=(1,0)) or (ch!=(0,1) and ch!=(1,0)):\n",
    "#                 fa = record.samples[father]['GT']\n",
    "#                 df.loc[len(df.index)] = [locus,mo,fa,ch,f0] # type: ignore\n",
    "#     num_row = df.shape[0]\n",
    "#     assert num_row>=n,'not enough locus to select'\n",
    "#     i = random.randint(n, num_row)\n",
    "#     df_select = df.iloc[i-n:i,:]\n",
    "#     df_select.set_index('locus',drop=True,inplace=True)\n",
    "#     return df_select\n",
    "'''\n",
    "\n",
    "def calcul_XY(df_select):\n",
    "    '''遍历选择的位点计算X,Y和c, 返回logX,logY and c'''\n",
    "    log_mu = -8 # 默认突变概率为1e-8\n",
    "    log2 = np.log10(2)\n",
    "\n",
    "    n = df_select.shape[0]\n",
    "    log_x = 0\n",
    "    log_y = 0\n",
    "    c = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        father = df_select['faGT'][i]\n",
    "        mother = df_select['moGT'][i]\n",
    "        child = df_select['chGT'][i]\n",
    "        if child == (0,0):\n",
    "            if mother == (0,1) or mother == (1,0):\n",
    "                log_y = log_y - log2\n",
    "                c.append(0)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - 2*log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x - log2\n",
    "                else:\n",
    "                    log_x = log_x + log_mu - log2\n",
    "\n",
    "            elif mother == (0,0):\n",
    "                c.append(0)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - log2\n",
    "                elif father == (0,0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_x = log_x + log_mu\n",
    "\n",
    "            else:\n",
    "                log_y = log_y + log_mu\n",
    "                c.append(0)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x + log_mu -log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x + log_mu\n",
    "                else:\n",
    "                    log_x = log_x + 2*log_mu\n",
    "\n",
    "        elif child == (0,1) or child == (1,0):\n",
    "            if mother == (0,1) or mother == (1,0):\n",
    "                c.append(0)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x - log2\n",
    "                else:\n",
    "                    log_x = log_x - log2\n",
    "            #    assert 1==0,'mother and child are both heteGT'\n",
    "\n",
    "            elif mother == (0,0):\n",
    "                c.append(1)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x + log_mu\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                c.append(0)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - log2\n",
    "                elif father == (0,0):\n",
    "                    pass\n",
    "                else:\n",
    "                    log_x = log_x + log_mu\n",
    "\n",
    "        else:\n",
    "            if mother == (0,1) or mother == (1,0):\n",
    "                log_y = log_y - log2\n",
    "                c.append(1)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - 2*log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x + log_mu - log2\n",
    "                else:\n",
    "                    log_x = log_x -log2\n",
    "\n",
    "            elif mother == (0,0):\n",
    "                log_y = log_y + log_mu\n",
    "                c.append(1)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x + log_mu - log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x + 2* log_mu\n",
    "                else:\n",
    "                    log_x = log_x + log_mu\n",
    "\n",
    "            else:\n",
    "                c.append(1)\n",
    "                if father == (0,1) or father == (1,0):\n",
    "                    log_x = log_x - log2\n",
    "                elif father == (0,0):\n",
    "                    log_x = log_x + log_mu\n",
    "                else:\n",
    "                    pass\n",
    "    c = np.array(c)\n",
    "    # print('X,Y,len_c:',log_x,log_y,c.shape)\n",
    "    return log_x,log_y,c\n",
    "\n",
    "\n",
    "def calcul_pi_ind(df_select,log_x,log_y,c):\n",
    "    '''计算独立情况的亲权系数'''\n",
    "    f0 = df_select['f0'].values\n",
    "    pr_ind = (np.log10(np.where(c==0,f0,1-f0))).sum() # type: ignore\n",
    "    return log_x - log_y - pr_ind\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def freq2(x,y):\n",
    "    '''返回bool列表,True表示x,y对应位置同时为0'''\n",
    "    return (x==0)&(y==0)\n",
    "\n",
    "def calcul_fij(df_data,fi,locus_list,n):\n",
    "    num_row = df_data.shape[0]\n",
    "    fij = np.zeros((n,n))\n",
    "    for i in range(n):  # 因为fij对称，只遍历上三角矩阵\n",
    "        col1 = df_data[locus_list[i]].to_numpy().astype(int)   # 这里类型的变换，是为了使用numba加速\n",
    "        for j in range(i+1,n):\n",
    "            col2 = df_data[locus_list[j]].to_numpy().astype(int)\n",
    "            fij[i,j] = freq2(col1,col2).sum()/num_row  # 统计两列数据同时为０的概率\n",
    "            fij[j,i] = fij[i,j]\n",
    "    for i in range(n):\n",
    "        fij[i,i] = fi[i]\n",
    "    return fij\n",
    "\n",
    "def freq_pc(df_data,locus_list,fi,n,pc):\n",
    "    fij = calcul_fij(df_data,fi,locus_list,n)\n",
    "    return (1-pc)*fi + pc/2,(1-pc)*fij + pc/4\n",
    "\n",
    "def cMat(fi,fij,n,alpha):\n",
    "    cmat = fij - fi*(fi.reshape(n,1))\n",
    "    print('\\t c的最小特征值',np.min(np.linalg.eigvals(cmat)))\n",
    "    cmat = np.where(cmat>alpha,cmat,0)\n",
    "    return cmat\n",
    "\n",
    "def inv_eig(cmat):\n",
    "    D,V = np.linalg.eig(cmat)\n",
    "    lambda_inv= np.diag(list(map(lambda x: x.real/(0.001+x.real**2), D)))\n",
    "    inv_C2 = np.dot(np.dot(V, lambda_inv), V.T).real\n",
    "    return inv_C2\n",
    "\n",
    "def invc_glassoR(cmat,rho1,rho2):\n",
    "    c_path = '/home/anran/paternity/version4/cmat.txt'\n",
    "    invc_path = '/home/anran/paternity/version4/invc.txt'\n",
    "    invg_path = \"/home/anran/paternity/version4/invc_glasso.csv\"\n",
    "    np.savetxt(c_path,cmat)\n",
    "    np.savetxt(invc_path,inv_eig(cmat))\n",
    "    r_path = f\"/home/anran/paternity/version4/invc.R\"\n",
    "    r_command = ['Rscript',r_path]\n",
    "    r_command.extend(map(str,[c_path, invc_path, invg_path,rho1, rho2]))\n",
    "    subprocess.run(r_command,check=True)\n",
    "    invc = pd.read_csv(invg_path,sep=' ').values\n",
    "    if(os.path.isfile(c_path)):  # 删除保存的数据文件\n",
    "        os.remove(c_path)\n",
    "        os.remove(invc_path)\n",
    "        os.remove(invg_path)\n",
    "    return invc\n",
    "\n",
    "\n",
    "def calcul_e(invc,fi,n):\n",
    "    res = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            a = 2*fi[i]*fi[j]\n",
    "            delta = 1-4*a*invc[i, j]\n",
    "            if delta<0 or fi[i]==0 or fi[j]==0:\n",
    "                res[i,j] = -invc[i,j]\n",
    "            elif delta==0:\n",
    "                res[i,j] =  -1/(2*a)\n",
    "            else:\n",
    "                s1 = (-1+math.sqrt(delta))/(2*a)\n",
    "                s2 = (-1+math.sqrt(delta))/(2*a)\n",
    "                if abs(s1-invc[i,j])>abs(s2-invc[i,j]):  # 当有2实根，选择距-invc较近的根\n",
    "                    res[i,j] = s2\n",
    "                else:\n",
    "                    res[i,j] = s1\n",
    "    print('\\t emat:',res.min(),res.max())\n",
    "    return res\n",
    "\n",
    "\n",
    "def margin(fi,emat,n):\n",
    "    e = deepcopy(emat)\n",
    "    for i in range(n):  # h计算时，对j!=i的eij项操作\n",
    "        e[i,i] = 0\n",
    "    h = np.arctanh(fi) + (e**2 * fi.reshape(n,1) * (1-fi**2)).sum(axis=1) - (e*fi).sum(axis=1)\n",
    "    print('\\t h:',h.min(),h.max())\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_b(k):\n",
    "    '''根据k生成b, 维度为(2^k,k), 一行是一个样本'''\n",
    "    num_rows = 2**k  # 矩阵b的行数\n",
    "    b = np.zeros((num_rows, k), dtype=int)\n",
    "    for i in range(num_rows):\n",
    "        binary_representation = format(i, '0' + str(k) + 'b')  # 将整数i转换为k位的二进制字符串\n",
    "        for j in range(k):\n",
    "            b[i, j] = int(binary_representation[j])\n",
    "    return b\n",
    "\n",
    "def select_emat(seq,j,emat,k):\n",
    "    '''对于第j个位点, 只考虑它及其前面的k-1个位点\n",
    "    取出emat中的子方阵[j-k+1.j-k+1]->[j,j]\n",
    "    根据seq计算对应的系数(1 or -1)'''\n",
    "    s = np.tile(seq,(k,1))\n",
    "    mask = np.logical_not(s ^ s.T)  # 00/11->1,01/10->0\n",
    "    mask = np.where(mask==0,-1,mask)  \n",
    "    return np.triu(emat[j-k+1:j+1,j-k+1:j+1]*mask,1)\n",
    "\n",
    "def get_addition(j,k,b,e,h):\n",
    "    '''返回E向量, 维度(2**k,)'''\n",
    "    coef = np.zeros(2**k)\n",
    "    for i in range(2**k):\n",
    "        seq = b[i,:]\n",
    "        mask = np.where(np.logical_not(seq[-1]^seq[:-1]),1,-1)\n",
    "        coef[i] = (e[j,j-k+1:j] * mask).sum() + h[j]*(-1)**seq[-1]\n",
    "    return coef\n",
    "\n",
    "def calcul_Z(e,h,k,n):\n",
    "    z = np.zeros((2**k,n-k+1))\n",
    "    b = get_b(k)\n",
    "\n",
    "    for i in range(2**k):  # 计算ｚ的第一列数据z[0]\n",
    "        seq = b[i,:]\n",
    "        z[i,0] = (h[:k]*np.where(seq==0,1,-1)).sum()+select_emat(seq,k-1,e,k).sum()\n",
    "    \n",
    "    for l in range(1,n-k+1): # 循环计算z[1]到z[n-k], l指向z的index, 对应e/h的index为 l+(k-1)\n",
    "        coef = get_addition(l+k-1,k,b,e,h)\n",
    "        for i in range(2**k):\n",
    "            seq = b[i,:]\n",
    "            l0 = (i-seq[-1]) // 2  # 二进制数右移移位，最高位为0\n",
    "            l1 = (l0 + 2**(k-1)) # 二进制数右移移位，最高位为１\n",
    "            z0 = min(z[l0,l-1],z[l1,l-1])\n",
    "            z1 = max(z[l0,l-1],z[l1,l-1])\n",
    "            if z1-z0>200:\n",
    "                z[i,l] = coef[i] + z1\n",
    "            else: \n",
    "                z[i,l] = coef[i] + z0 + np.log(np.exp(1)+np.exp(z1-z0))\n",
    "    z_min = z[:,-1].min()\n",
    "    log10_z = np.log10(np.exp(z[:,-1]-z_min).sum()) + np.log10(np.exp(1))*z_min\n",
    "    return log10_z\n",
    "\n",
    "def calcul_num(c, e, h, n, k):\n",
    "    '''输入e维度N*N\n",
    "    返回10的指数部分'''\n",
    "    nums = np.zeros(n-k+1)\n",
    "    sum_h = (h*np.where(c==0,1,-1)).sum()\n",
    "\n",
    "    nums[0] = select_emat(c[:k],k-1,e,k).sum()\n",
    "\n",
    "    for j in range(k,n):\n",
    "        l = j-k+1  # nums中的index\n",
    "        mask = np.where(np.logical_not(c[j]^c[l:j]),1,-1)  # 使用c_j与其前k-1个位点计算mask\n",
    "        coef = (e[j,l:j] * mask).sum()\n",
    "        nums[l] = coef + nums[l-1]\n",
    "    return (nums[-1]+sum_h)*np.log10(np.exp(1))\n",
    "\n",
    "\n",
    "# def get_fi(vcf_file,n):\n",
    "#     '''select N locus from vcf_file'''\n",
    "#     vcf = pysam.VariantFile(vcf_file)\n",
    "#     f0=[]\n",
    "#     for record in vcf:\n",
    "#         f1 = record.info['AF'][0]\n",
    "#         locus = (record.chrom,str(record.pos))\n",
    "#         if f1<0.8 and f1>0.2:\n",
    "#             f0.append((locus,1-f1))\n",
    "#     l = random.randint(0,len(f0)-n)\n",
    "#     freq0 = dict(f0[l:l+n])\n",
    "#     fi = pd.Series(freq0)\n",
    "#     return fi\n",
    "\n",
    "def get_fi(df_data,start,end):\n",
    "    f0 = {}\n",
    "    totle = df_data.shape[0]\n",
    "    columns = df_data.columns.to_list()\n",
    "    print('linkage aera: ',columns[start],' to ', columns[end])\n",
    "    for i in range(start,end+1):\n",
    "        locus = columns[i]\n",
    "        freq = (df_data[locus]==0).sum()/totle\n",
    "        f0[locus] = freq\n",
    "    fi = pd.Series(f0)\n",
    "    return fi\n",
    "\n",
    "\n",
    "\n",
    "def get_data(locus_list, father, mother, child):\n",
    "    '''根据locus_list抽取人的数据'''\n",
    "    df = pd.DataFrame(columns=['locus','moGT','faGT', 'chGT','f0'])\n",
    "    for record in vcf:   # 先将所有数据读进df\n",
    "        locus = (record.chrom,str(record.pos))\n",
    "        f0 = 1-record.info['AF'][0]\n",
    "        if locus in locus_list:  # 除去f0=1的情况，不然会导致之后 arctanh(1)=inf\n",
    "            mo = record.samples[mother]['GT']\n",
    "            ch = record.samples[child]['GT']\n",
    "            fa = record.samples[father]['GT']\n",
    "            df.loc[len(df.index)] = [locus,mo,fa,ch,f0] # type: ignore\n",
    "    df.set_index('locus',drop=True,inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = 0.001\n",
    "alpha = 0.001\n",
    "rho1 = 0.5\n",
    "rho2 = 0.1\n",
    "data_path = \"/home/anran/paternity/family-data/sim_seg1\" \n",
    "hap_file,samples_file,vcf_file,ped_file=get_file(data_path)\n",
    "persons = pd.read_table(ped_file)\n",
    "df_data_init = convert_hap_samples_to_dataframe(hap_file,samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('/home/anran/paternity/family-data/sim_data/data1.csv')\n",
    "df_data.drop(['Unnamed: 0','personID'], axis=1,inplace=True)\n",
    "# df_data2 = pd.read_csv('/home/anran/paternity/family-data/sim_data/data2.csv')\n",
    "# df_data = pd.concat([df_data1,df_data2])\n",
    "df_data.columns = df_data_init.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkage aera:  ('chr6', '22603023')  to  ('chr6', '22651774')\n"
     ]
    }
   ],
   "source": [
    "with open('/home/anran/paternity/family-data/sim_data/info1.json') as f:\n",
    "    data = json.load(f)\n",
    "start = data['start']\n",
    "end = data['end']\n",
    "fi_serie = get_fi(df_data,start,end)\n",
    "N = fi_serie.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locus_list = fi_serie.index.to_list()\n",
    "fi =  fi_serie.values\n",
    "# fij = calcul_fij(df_data,fi,locus_list,N)\n",
    "fi,fij = freq_pc(df_data,locus_list,fi,N,pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t c的最小特征值 (-5.80599909613751e-15+0j)\n"
     ]
    }
   ],
   "source": [
    "cmat = cMat(fi,fij,N,alpha)  \n",
    "invc = invc_glassoR(cmat,rho1,rho2)\n",
    "diag = ((invc-np.diag(np.diagonal(invc)))==0).all()  # 判断逆矩阵是否为对角矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t emat: -2.0 0.24730314984745916\n",
      "\t h: 0.18322466855360606 4.146899804423353\n"
     ]
    }
   ],
   "source": [
    "emat = calcul_e(invc,fi,N) \n",
    "h = margin(fi,emat,N)\n",
    "log_z = calcul_Z(emat,h,K,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        , -2.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -1.33516545, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -2.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -2.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        , -2.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fi_ind(df_data,start,end,n):\n",
    "    f0={}\n",
    "    totle = df_data.shape[0]\n",
    "    num_col = df_data.shape[1]\n",
    "    # index_available = np.append(np.arange(start),np.arange(end+1,num_col))\n",
    "    # print(index_available.shape)\n",
    "    index = np.random.choice(np.append(np.arange(start),np.arange(end+1,num_col)),size=n,replace=False)\n",
    "    columns = df_data.columns.to_list()\n",
    "    for i in index:\n",
    "        locus = columns[i]\n",
    "        freq = (df_data[locus]==0).sum()/totle\n",
    "        f0[locus] = freq\n",
    "    fi = pd.Series(f0)\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vcf \u001b[39m=\u001b[39m pysam\u001b[39m.\u001b[39mVariantFile(vcf_file)\n\u001b[0;32m----> 2\u001b[0m df_select \u001b[39m=\u001b[39m get_data(locus_list,vcf,persons\u001b[39m.\u001b[39mfather[i],persons\u001b[39m.\u001b[39mmother[j],persons\u001b[39m.\u001b[39mchild[j])\n\u001b[1;32m      3\u001b[0m log_x,log_y,c \u001b[39m=\u001b[39m calcul_XY(df_select)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "vcf = pysam.VariantFile(vcf_file)\n",
    "df_select = get_data(locus_list,vcf,persons.father[i],persons.mother[j],persons.child[j])\n",
    "log_x,log_y,c = calcul_XY(df_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linkage_intervals(num_linkage,num_locus,num_keys):\n",
    "    '''生成给定数量的重组间隔，即随机选择一定数量的变异位点作为重组点，形成相邻两个变异位点之间的重组区间。'''\n",
    "    linkage_init = random.randint(0,num_locus-num_linkage)\n",
    "    keys = np.random.choice(np.arange(1,num_linkage-1), num_keys,replace=False) + linkage_init\n",
    "    linkage_keys = np.sort(np.append(keys,[linkage_init,linkage_init+num_linkage]))\n",
    "    return linkage_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 11, 14, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkage_keys = generate_linkage_intervals(8,20,2)\n",
    "linkage_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  8,  8, 11, 11, 11, 14, 14, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.arange(20)\n",
    "for j in range(20):\n",
    "    if j<linkage_keys[0] or j>=linkage_keys[-1]:\n",
    "        pass           \n",
    "    else:\n",
    "        index = np.searchsorted(linkage_keys,j,side='right') - 1\n",
    "        temp[j] = temp[linkage_keys[index]]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(map(int,a))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2\n",
      "0   1   2   3\n",
      "1   4   5   6\n",
      "2   7   8   9\n",
      "3  10  11  12\n"
     ]
    }
   ],
   "source": [
    "lst=[[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12]]\n",
    "df=pd.DataFrame(lst)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "person_data = pd.DataFrame(columns = np.arange(df.shape[1]))\n",
    "for j in range(df.shape[0]//2):\n",
    "    print(j)\n",
    "    person_data.loc[j] = list(zip(df.iloc[2*j].values,df.iloc[2*j+1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>(3, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(7, 10)</td>\n",
       "      <td>(8, 11)</td>\n",
       "      <td>(9, 12)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2\n",
       "0   (1, 4)   (2, 5)   (3, 6)\n",
       "1  (7, 10)  (8, 11)  (9, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from re import match\n",
    "import pysam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from convert import convert_hap_samples_to_dataframe\n",
    "\n",
    "def get_file(data_path):\n",
    "    '''从文件夹中，得到各文件的路径'''\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            hap_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.sample'):\n",
    "            samples_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.vcf'):\n",
    "            vcf_file = os.path.join(data_path,filename)\n",
    "        if filename.endswith('.ped'):\n",
    "            ped_file = os.path.join(data_path,filename)\n",
    "    assert (hap_file and samples_file and vcf_file and ped_file),'missing data file'\n",
    "    return  hap_file,samples_file,vcf_file,ped_file\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simulate generations of families and populations\")\n",
    "    parser.add_argument(\"dataset\", help=\"Input data file with initial individuals\")\n",
    "    parser.add_argument(\"output_file_path\", help=\"path for saving simulation files \")\n",
    "    parser.add_argument(\"-g\", \"--num_generations\", type=int, default=3, help=\"Number of generations to simulate (default: 3)\")\n",
    "    parser.add_argument(\"-c\", \"--num_couples\", type=int, default=10, help=\"Number of initial couples (default: 10)\")\n",
    "    parser.add_argument(\"-l\", \"--num_linkages\", type=int, default=1000, help=\"Number of consecutive non-recombining positions (default: 1000)\")\n",
    "    parser.add_argument(\"-p\", \"--num_keys\", type=int, default=0, help=\"key points for linkage interval (default: 0)\")\n",
    "    parser.add_argument(\"-k\", \"--num_kids\", type=int, default=2, help=\"Number of child for every couple (default: 2)\")\n",
    "    parser.add_argument(\"-r\", \"--num_recombinations\", type=int, default=5, help=\"Number of intervals for recombination (default: 5)\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def choice_person(df_data,num_couples,linkage_keys):\n",
    "    start = linkage_keys[0]\n",
    "    end = linkage_keys[-1]\n",
    "    res0 = pd.DataFrame()\n",
    "    select_people = {}\n",
    "    num_people = df_data.shape[0]//2\n",
    "    person_list = df_data.index.tolist()\n",
    "    df_data.columns = np.arange(df_data.shape[1])\n",
    "    for i in range(2*num_couples):\n",
    "        index = random.randint(0,num_people)\n",
    "        select_people[i] = person_list[2*index]\n",
    "        select_data = deepcopy(df_data.iloc[index*2:index*2+2])\n",
    "        select_data['personID'] = [i,i]\n",
    "        res0 = pd.concat([res0,select_data])\n",
    "    res0.set_index(np.arange(4*num_couples),drop=True,inplace=True)\n",
    "    for i in range(res0.shape[0]):\n",
    "        res0.iloc[i,start:end] = res0.iloc[i,start]\n",
    "    return res0,select_people\n",
    "\n",
    "def generate_linkage_intervals(num_linkage,num_locus,num_keys=0):\n",
    "    linkage_init = random.randint(0,num_locus-num_linkage)\n",
    "    keys = np.random.choice(np.arange(1,num_linkage-1), num_keys,replace=False) + linkage_init\n",
    "    linkage_keys = np.sort(np.append(keys,[linkage_init,linkage_init+num_linkage+1]))\n",
    "    return linkage_keys\n",
    "\n",
    "def generate_recombination_intervals(num_recombinations,num_locus,linkage_keys):\n",
    "    start = linkage_keys[0]\n",
    "    end = linkage_keys[-1]\n",
    "    num_linkage = end-start\n",
    "    interval1 = range(0,start)\n",
    "    interval2 = range(end+1,num_locus+1)\n",
    "    points1 = start*num_recombinations//(num_locus-num_linkage)\n",
    "    points2 = num_recombinations - points1\n",
    "    recombination_points1 = sorted(random.sample(interval1, 2 * points1)) \n",
    "    recombination_points2 = sorted(random.sample(interval2, 2 * points2))\n",
    "    recombination_points = recombination_points1 + recombination_points2\n",
    "    recombination_intervals = [(recombination_points[i], recombination_points[i + 1]) for i in range(0, len(recombination_points), 2)]\n",
    "    return recombination_intervals\n",
    "\n",
    "def is_variant_in_recombination_intervals(variant_pos, recombination_intervals):\n",
    "    '''检查一个变异位点是否在给定的重组区间内'''\n",
    "    if recombination_intervals!=[]:\n",
    "        for start, end in recombination_intervals:\n",
    "            if start <= variant_pos <= end:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def sim_meosis(res,num_recombinations,linkage_keys):\n",
    "    num_locus = res.shape[1]-1\n",
    "    start = linkage_keys[0]\n",
    "    end = linkage_keys[-1]\n",
    "    temp = deepcopy(res[res.index%2==0])\n",
    "    for i in range(temp.shape[0]):\n",
    "        if num_recombinations>0:\n",
    "            recombination_intervals = generate_recombination_intervals(num_recombinations,num_locus,linkage_keys)\n",
    "        else:\n",
    "            recombination_intervals = []\n",
    "        for j in range(num_locus):\n",
    "            if j<start or j>=end:\n",
    "                if is_variant_in_recombination_intervals(j,recombination_intervals):\n",
    "                    temp.iloc[i,j] = res.iloc[i+1,j]\n",
    "            # else:\n",
    "            #     # index = np.searchsorted(linkage_keys,j,side='right') - 1   # type: ignore\n",
    "            #     temp.iloc[i,j] = res.iloc[i,start]\n",
    "\n",
    "    return temp\n",
    "\n",
    "def couple_legal(person1,person2,family=None):\n",
    "    if not family:\n",
    "        return True\n",
    "    else:\n",
    "        if person1 in family.keys() and person2 in family.keys():\n",
    "            if family[person1] == family[person2]:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def select_parent(person_rest,family):\n",
    "    persons = deepcopy(person_rest)\n",
    "    parent1,parent2 = random.sample(persons,2)\n",
    "    while not couple_legal(parent1,parent2,family):\n",
    "        parent1,parent2 = random.sample(persons,2)\n",
    "    persons.remove(parent1)\n",
    "    persons.remove(parent2)\n",
    "    return persons,parent1,parent2\n",
    "\n",
    "def valid_linkage(df,linkage_keys):\n",
    "    for i in range(len(linkage_keys)-1):\n",
    "        start = linkage_keys[i]\n",
    "        end = linkage_keys[i+1]\n",
    "        a = (df.iloc[:,start]=='0').sum()\n",
    "        for j in range(start,end):\n",
    "            if (df.iloc[:,j]=='0').sum() != a:\n",
    "                print('不满足强连锁，列数：',i)\n",
    "                return False\n",
    "    return True\n",
    "                \n",
    "def genetype(df_family,dict_family):\n",
    "    family = df_family.drop(['personID'], axis=1)\n",
    "    person_data = pd.DataFrame(columns = np.arange(family.shape[1]))\n",
    "    print(family.shape,person_data.shape)\n",
    "    for j in range(family.shape[0]//2):\n",
    "        print(j)\n",
    "        person_data.loc[j] = list(zip(family.iloc[2*j].values,family.iloc[2*j+1].values))\n",
    "\n",
    "    person_data.to_csv('/home/anran/paternity/family-data/sim_data/person.csv',index=False)\n",
    "\n",
    "    person = []\n",
    "    for key in dict_family.keys():\n",
    "        if key != 'linkage_keys':\n",
    "            person.append([int(key)]+list(map(int,family[key])))\n",
    "    persons = pd.DataFrame(person,columns = ['child','father','mother'])\n",
    "    persons.to_csv('/home/anran/paternity/family-data/sim_data/ped.csv',index=False)\n",
    "    return 0\n",
    "\n",
    "def dataset(df_data,linkage_keys,args):\n",
    "    num_generation = args.num_generations\n",
    "    num_couple = args.num_couples\n",
    "    num_child = args.num_kids\n",
    "    num_recombination = args.num_recombinations\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    res0,family = choice_person(df_data,num_couple,linkage_keys)\n",
    "    assert valid_linkage(res0,linkage_keys),'dataset select data non linkage'\n",
    "    result_df = pd.concat([result_df,res0])\n",
    "    personID = 2*num_couple\n",
    "    for g in range(num_generation):\n",
    "        print('#####################',g)\n",
    "        couple = []\n",
    "        for k in range(num_child):\n",
    "            print('#########',k)\n",
    "            temp = sim_meosis(res0,num_recombination,linkage_keys)\n",
    "            persons_rest = list(map(int,list(temp['personID'].values)))\n",
    "            # print(temp.shape,persons_rest)\n",
    "            if k==0:\n",
    "                while persons_rest!=[]:\n",
    "                    persons_rest,parent1,parent2 = select_parent(persons_rest,family)\n",
    "                    # print('-----------------',parent1,parent2,persons_rest)\n",
    "                    couple.append((parent1,parent2))\n",
    "                    family[personID] = [parent1,parent2]\n",
    "                    res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "                    res_temp['personID'] = [personID,personID]\n",
    "                    result_df = pd.concat([result_df,res_temp])\n",
    "                    personID += 1\n",
    "            else:\n",
    "                for (parent1,parent2) in couple:\n",
    "                    family[personID] = [parent1,parent2]\n",
    "                    res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "                    res_temp['personID'] = [personID,personID]\n",
    "                    result_df = pd.concat([result_df,res_temp])\n",
    "                    personID += 1\n",
    "        res0 = result_df.iloc[2*personID-4*num_couple:2*personID]\n",
    "        res0.set_index(np.arange(res0.shape[0]),drop=True,inplace=True)\n",
    "\n",
    "    assert valid_linkage(result_df,linkage_keys),'dataset final data non linkage'\n",
    "    family['linkage_keys'] = list(map(int,linkage_keys))\n",
    "    result_df.to_csv(args.output_file_path + 'df_data.csv',index=False)\n",
    "    with open(args.output_file_path+\"df_data.json\",\"w\",encoding='utf-8') as f:\n",
    "        json.dump(family,f)\n",
    "    return 0\n",
    "\n",
    "def test_dataset(df_data,linkage_keys,args):\n",
    "    num_couple = 5\n",
    "    num_recombination = args.num_recombinations\n",
    "    result_df = pd.DataFrame()\n",
    "    family = {}\n",
    "\n",
    "    res0,_ = choice_person(df_data,num_couple,linkage_keys)\n",
    "    assert valid_linkage(res0,linkage_keys),'family select data non linkage'\n",
    "    result_df = pd.concat([result_df,res0])\n",
    "    personID = 2*num_couple\n",
    "   \n",
    "    temp = sim_meosis(res0,num_recombination,linkage_keys)\n",
    "    persons_rest = list(map(int,list(temp['personID'].values)))\n",
    "    # print(temp.shape,persons_rest)\n",
    "\n",
    "    while persons_rest!=[]:\n",
    "        persons_rest,parent1,parent2 = select_parent(persons_rest,family)\n",
    "        print('-----------------',parent1,parent2,persons_rest)\n",
    "        family[personID] = [parent1,parent2]\n",
    "        res_temp = deepcopy(temp[temp['personID'].isin([parent1,parent2])])\n",
    "        res_temp['personID'] = [personID,personID]\n",
    "        result_df = pd.concat([result_df,res_temp])\n",
    "        personID += 1\n",
    "\n",
    "    assert valid_linkage(result_df,linkage_keys),'family final data non linkage'\n",
    "    # result_df.to_csv(args.output_file_path + 'family.csv',index=False)\n",
    "    with open(args.output_file_path+\"family.json\",\"w\",encoding='utf-8') as f:\n",
    "        json.dump(family,f)\n",
    "    genetype(result_df,family)\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
